# -*- coding: utf-8 -*-
"""VRA_ADVANCED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IFDN08mQI3QmoOLDWRmkVxMnpn0cGAuf
"""

import pandas as pd
import numpy as np
data = pd.read_csv("onlinefraud.csv")
data.head()

data=data.dropna()
data.isnull().sum()

type = data["type"].value_counts()
transactions = type.index
quantity = type.values

import plotly.express as px
figure = px.pie(data,
             values=quantity,
             names=transactions,hole = 0.5,
             title="Distribution of Transaction Type")
figure.show()

data.dtypes

#Label encoding the nameOrig and nameDest columns just to check the correlation
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
data['nameOrig']=le.fit_transform(data['nameOrig'])
data['nameDest']=le.fit_transform(data['nameDest'])
dfnew=data.drop(columns=['isFlaggedFraud','nameOrig','nameDest','step'])
dfnew.to_csv("Cleaned_data.csv")
data['type']=le.fit_transform(data['type'])
data.head()

# Checking correlation
import seaborn as sns
import matplotlib.pyplot as plt
correlation = data.corr()
# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()
print(correlation["isFraud"].sort_values(ascending=False))

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# Standardize
scaled_data = StandardScaler().fit_transform(data)
# Apply PCA
pca = PCA(n_components=4)
pca_components = pca.fit_transform(scaled_data)

# Plot results
# Create a DataFrame with the PCA components
pca_df = pd.DataFrame(data=pca_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])

# Visualize pairwise relationships with a pair plot
sns.pairplot(pca_df)
plt.suptitle('Pair Plot of First Four Principal Components', y=1.02)
plt.show()

# Access loadings
loadings = pca.components_

# Create DataFrame for loadings
loadings_df = pd.DataFrame(loadings.T, columns=['PC1', 'PC2', 'PC3','PC4'], index=data.columns)

# Display loadings
print("Loadings:")
print(loadings_df)

#Looking at the contribution to principle components we can drop 'isFlaggedFraud' and 'nameOrig'
data.drop(columns=['isFlaggedFraud','nameOrig','nameDest','step'],inplace=True)
data.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Separate features and target variable
X = data.drop(columns=['isFraud'])
y = data['isFraud']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

"""#We can clearly see that selecting all columns will yield best results
##LOGISTIC REGRESSION

"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,precision_score
# Feature selection using SelectKBest
for i in range(1, X.shape[1]+1):
  k_best = SelectKBest(score_func=f_classif, k=i)  # Select i best features
  X_train_selected = k_best.fit_transform(X_train_scaled, y_train)
  X_test_selected = k_best.transform(X_test_scaled)

# Get selected feature indices
  selected_feature_indices = k_best.get_support(indices=True)

# Get names of selected features
  selected_features = X.columns[selected_feature_indices]

# Train a model using selected features
  lr = LogisticRegression()
  lr.fit(X_train_selected, y_train)
  y_pred = lr.predict(X_test_selected)
  y_pred_train = lr.predict(X_train_selected)
# Evaluate model performance
  accuracy = accuracy_score(y_test, y_pred)
  accuracy_train = accuracy_score(y_train, y_pred_train)

  print("Accuracy of test data", accuracy)
  # precision = precision_score(y_test, y_pred)
  # precision_train = precision_score(y_train, y_pred_train)


  # print("Precision of test data:", precision)
  print("Accuracy of train data:", accuracy_train)
  # print("Precision of train data:", precision_train)
# Print names of selected features
  print("Selected features:", selected_features)

# Train a model using selected features
lrfinal = LogisticRegression()
lrfinal.fit(X_train_scaled, y_train)
y_pred = lrfinal.predict(X_test_scaled)
y_pred_train = lrfinal.predict(X_train_scaled)
# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
accuracy_train = accuracy_score(y_train, y_pred_train)
print("Accuracy of test data", accuracy)
print("Accuracy of train data:", accuracy_train)

"""##LINEAR REGRESSION"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
lir = LinearRegression()
lir.fit(X_train_scaled, y_train)
y_pred = lir.predict(X_test_scaled)
y_pred_train=lir.predict(X_train_scaled)
# Evaluate model performance
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
mse_train = mean_squared_error(y_train, y_pred_train)
print("Mean Squared Error (train):", mse_train)

"""##DECISION TREE

"""

from sklearn.tree import DecisionTreeClassifier
# Initialize and fit the model
model = DecisionTreeClassifier()
model.fit(X_train_scaled, y_train)

# Predict on the testing set
y_pred_test = model.predict(X_test_scaled)

# Calculate accuracy for testing set
accuracy_test = accuracy_score(y_test, y_pred_test)
print("Accuracy: for testing data", accuracy_test)

# Predict on the training set
y_pred_train = model.predict(X_train_scaled)

# Calculate accuracy for the training set
accuracy_train = accuracy_score(y_train, y_pred_train)

print("Accuracy for training data:", accuracy_train)

from sklearn.model_selection import cross_val_score, KFold

# Define the number of folds for cross-validation
num_folds = 5

# Define the cross-validation strategy
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean Accuracy:", cv_scores.mean())
print("Standard Deviation of Accuracy:", cv_scores.std())

"""##RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

# Initialize and fit the model
rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)

# Predict on the testing set
y_pred_test_rf = rf_model.predict(X_test_scaled)

# Calculate accuracy for the testing set
accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)
print("Accuracy for testing data:", accuracy_test_rf)

# Predict on the training set
y_pred_train_rf = rf_model.predict(X_train_scaled)

# Calculate accuracy for the training set
accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)
print("Accuracy for training data:", accuracy_train_rf)

from sklearn.model_selection import cross_val_score, KFold

# Define the number of folds for cross-validation
num_folds = 5

# Define the cross-validation strategy
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=kfold, scoring='accuracy')

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean Accuracy:", cv_scores.mean())
print("Standard Deviation of Accuracy:", cv_scores.std())

"""##GradientBoostingClassifier

"""

from sklearn.ensemble import GradientBoostingClassifier

# Initialize and fit the model
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train_scaled, y_train)

# Predict on the testing set
y_pred_test_gb = gb_model.predict(X_test_scaled)

# Calculate accuracy for the testing set
accuracy_test_gb = accuracy_score(y_test, y_pred_test_gb)
print("Accuracy for testing data:", accuracy_test_gb)

# Predict on the training set
y_pred_train_gb = gb_model.predict(X_train_scaled)

# Calculate accuracy for the training set
accuracy_train_gb = accuracy_score(y_train, y_pred_train_gb)
print("Accuracy for training data:", accuracy_train_gb)

import pickle
pickle.dump(le,open('label_encoder.pkl','wb'))
pickle.dump(gb_model,open('gbmodel.pkl','wb'))
pickle.dump(rf_model,open('rfmodel.pkl','wb'))
pickle.dump(lir,open('lirmodel.pkl','wb'))
pickle.dump(lrfinal,open('lrmodel.pkl','wb'))
pickle.dump(model,open('dtmodel.pkl','wb'))

gbmodel=pickle.load(open('gbmodel.pkl','rb'))
rfmodel=pickle.load(open('rfmodel.pkl','rb'))
lirmodel=pickle.load(open('lirmodel.pkl','rb'))
lrmodel=pickle.load(open('lrmodel.pkl','rb'))
dtmodel=pickle.load(open('dtmodel.pkl','rb'))
label_encoder = pickle.load(open('label_encoder.pkl','rb'))
data = pd.DataFrame([[3,1864.28,21249.0,19384.72,0.0,0.0]], columns=["type","amount","oldbalanceOrg","newbalanceOrig","oldbalanceDest","newbalanceDest"])
rfprediction = rfmodel.predict(data)
gbprediction = gbmodel.predict(data)
lirprediction = lirmodel.predict(data)
lrprediction = lrmodel.predict(data)
dtprediction = dtmodel.predict(data)
print(rfprediction)
print(gbprediction)
print(lirprediction)
print(lrprediction)
print(dtprediction)

